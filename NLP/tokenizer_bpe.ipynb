{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import pickle\n",
    "from re import sub\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from custom_transformer import CustomTransformer\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLoader:\n",
    "    def __init__(self, PATH) -> None:\n",
    "        \n",
    "        self.corpus = open(PATH, 'r').read()\n",
    "        self.sentences = self.corpus.split('\\n')\n",
    "\n",
    "    def remove_rare_words(self,vocab_dict, min_repeat):\n",
    "\n",
    "        wordList = [k for k,v in vocab_dict.items() if v >= min_repeat]\n",
    "        vocab = set(wordList)\n",
    "        print(len(vocab), len(wordList))\n",
    "\n",
    "    \n",
    "    def get_all_chars(self):\n",
    "        chars = defaultdict(lambda: 0)\n",
    "        for sen in self.sentences:\n",
    "            for char in sen:\n",
    "                chars[char] += 1\n",
    "\n",
    "        return chars\n",
    "\n",
    "    def remove_sentence_with_rare_chars(self, min_count, dict):\n",
    "        sens_idx = []\n",
    "        for idx, sen in enumerate(self.sentences):\n",
    "            flag = 1\n",
    "            for char in sen:\n",
    "                if(dict[char] < min_count):\n",
    "                    flag = 0\n",
    "                    break\n",
    "            if flag:\n",
    "                sens_idx.append(idx)\n",
    "        return sens_idx\n",
    "\n",
    "    def create_cleaned_data(self, idxs, ext):\n",
    "        cleaned_sentences = [self.sentences[i] for i in idxs]\n",
    "        self.sentences = cleaned_sentences\n",
    "        print(len(self.sentences))\n",
    "\n",
    "        clean_file = open('cleaned_train.' + ext, 'w+')\n",
    "        for i in range(len(self.sentences)):  \n",
    "            clean_file.write(self.sentences[i] + '\\n')\n",
    "            \n",
    "    def batch_iterator(self, batch_size):\n",
    "        for i in range(0, len(self.sentences), batch_size):\n",
    "            yield self.sentences[i : i + batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader_en = TextLoader('train.en')\n",
    "# loader_de = TextLoader('train.de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4442493, 4453061, 4449785)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chars_en = loader_en.get_all_chars()\n",
    "# chars_de = loader_de.get_all_chars()\n",
    "\n",
    "# chars = defaultdict(lambda:0)\n",
    "# for k,v in chars_en.items():\n",
    "#     chars[k] += v\n",
    "# for k,v in chars_de.items():\n",
    "#     chars[k] += v\n",
    "\n",
    "# chars = dict(sorted(chars.items(), key=lambda item: item[1], reverse=True))\n",
    "# # dict(sorted(chars_de.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# cleaned_en = loader_en.remove_sentence_with_rare_chars(1000, chars)\n",
    "# cleaned_de = loader_de.remove_sentence_with_rare_chars(1000, chars)\n",
    "# cleaned_idx = list(set(cleaned_en) & set(cleaned_de))\n",
    "# len(cleaned_idx), len(cleaned_en), len(cleaned_de)\n",
    "\n",
    "# loader_de.create_cleaned_data(cleaned_idx, 'de')\n",
    "# loader_en.create_cleaned_data(cleaned_idx, 'en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_en = TextLoader('cleaned_train.en')\n",
    "loader_de = TextLoader('cleaned_train.de')\n",
    "\n",
    "assert(len(loader_de.sentences) == len(loader_en.sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(models.BPE())\n",
    "tokenizer_en.normalizer = normalizers.Lowercase()\n",
    "# tokenizer_en.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "tokenizer_en.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "\n",
    "trainer_en = trainers.BpeTrainer(vocab_size=25000,special_tokens=[\"[PAD]\"])\n",
    "tokenizer_en.train_from_iterator(loader_en.batch_iterator(1000), trainer=trainer_en)\n",
    "\n",
    "# tokenizer_en.decoder =  decoders.ByteLevel()\n",
    "tokenizer_en.save(\"tokenizer_en_25000_start_token_SOS.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is bas u'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.decode([527, 1263, 197, 627, 48, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'name', 'is', 'bas', 'u'] [527, 1263, 197, 627, 48]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'my name is bas u'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer_en.encode(\"My name is Basu\")\n",
    "print(encoding.tokens,encoding.ids )\n",
    "tokenizer_en.decode(encoding.ids)\n",
    "\n",
    "for batch in loader_en.batch_iterator(2):\n",
    "    encoding = tokenizer_en.encode_batch(batch)\n",
    "    print(tokenizer_en.decode_batch([encoding[i].ids for i in range(len(encoding))]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for batch in loader_en.batch_iterator(1024):\n",
    "    encoding = tokenizer_en.encode_batch(batch)\n",
    "    for i in range(len(encoding)):\n",
    "        li.append(len(encoding[i].ids))\n",
    "\n",
    "li = sorted(li, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5000, 6000):\n",
    "    if li[i] <= 100:\n",
    "        print(i)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_de = Tokenizer(models.BPE())\n",
    "tokenizer_de.normalizer = normalizers.Lowercase()\n",
    "# tokenizer_de.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "tokenizer_de.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "\n",
    "# print(tokenizer.pre_tokenizer.pre_tokenize_str(\"Ich liebe das wirklich .\"))\n",
    "\n",
    "trainer_de = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"[PAD]\", \"[SOS]\", \"[EOS]\"])\n",
    "tokenizer_de.train_from_iterator(loader_de.batch_iterator(1000), trainer=trainer_de)\n",
    "\n",
    "tokenizer_de.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[SOS]:0 $A:0 [EOS]:0\",\n",
    "    special_tokens=[(\"[SOS]\", 0), (\"[EOS]\", 1)])\n",
    "# tokenizer_de.decoder =  decoders.ByteLevel()\n",
    "\n",
    "tokenizer_de.save(\"tokenizer_de_25000_start_token_SOS.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_de.decode([i for i in range(300)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[SOS]', 'my', 'name', 'is', 'bas', 'u', '[EOS]'] [0, 2985, 3725, 250, 1171, 50, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'my name is bas u'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer_de.encode(\"My name is Basu\")\n",
    "print(encoding.tokens, encoding.ids)\n",
    "tokenizer_de.decode(encoding.ids + [0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for batch in loader_de.batch_iterator(1024):\n",
    "#     encoding = tokenizer_de.encode_batch(batch)\n",
    "#     a = tokenizer_de.decode_batch([encoding[i].ids for i in range(len(encoding))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_de = Tokenizer.from_file(\"tokenizer_de_25000_start_token_SOS.json\")\n",
    "tokenizer_en = Tokenizer.from_file(\"tokenizer_en_25000_start_token_SOS.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_encodes = []\n",
    "encodes_len = []\n",
    "for idx, batch in enumerate(loader_en.batch_iterator(1024)):\n",
    "    encoding = tokenizer_en.encode_batch(batch)\n",
    "    for i in range(len(encoding)):\n",
    "        en_encodes.append(encoding[i].ids)\n",
    "        encodes_len.append(len(encoding[i].ids))\n",
    "\n",
    "de_encodes = []\n",
    "for idx, batch in enumerate(loader_de.batch_iterator(1024)):\n",
    "    encoding = tokenizer_de.encode_batch(batch)\n",
    "    for i in range(len(encoding)):\n",
    "        de_encodes.append(encoding[i].ids)\n",
    "\n",
    "encodes_len , en_encodes, de_encodes = (list(t) for t in zip(*sorted(zip(encodes_len , en_encodes, de_encodes))))\n",
    "\n",
    "\n",
    "# Dump the tokenized lists as pickle files for faster retrieval\n",
    "with open('en_tokenized.pkl', 'wb') as f:\n",
    "    pickle.dump(en_encodes, f)\n",
    "\n",
    "with open('de_tokenized.pkl', 'wb') as f:\n",
    "    pickle.dump(de_encodes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_tokenized.pkl', 'rb') as f:\n",
    "    en_tokens = pickle.load(f)\n",
    "\n",
    "with open('de_tokenized.pkl', 'rb') as f:\n",
    "    de_tokens = pickle.load(f)\n",
    "\n",
    "assert(len(de_tokens) == len(en_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(en_tokens, de_tokens, bs, pad_idx, shuffle = True):\n",
    "    num_batches = len(en_tokens)//bs\n",
    "    idxs = [i for i in range(num_batches)]\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(idxs)\n",
    "\n",
    "    for i in idxs:\n",
    "        max_len = len(en_tokens[(i+1)*bs - 1])\n",
    "        en_tensor = torch.tensor([enc + \n",
    "        [pad_idx]*(max_len - len(enc)) for enc in en_tokens[i*bs:(i+1)*bs]])\n",
    "        \n",
    "        en_pad_idx = torch.tensor((en_tensor != pad_idx)).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        max_len= max([len(enc) for enc in de_tokens[i*bs:(i+1)*bs]])\n",
    "        de_tensor = torch.tensor([enc + \n",
    "        [pad_idx]*(max_len - len(enc)) for enc in de_tokens[i*bs:(i+1)*bs]])\n",
    "\n",
    "        de_mask = torch.ones(bs, max_len, max_len).tril().unsqueeze(1)\n",
    "\n",
    "\n",
    "        yield en_tensor, en_pad_idx, de_tensor, de_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "nx = 6\n",
    "edim = 512\n",
    "hdim = 2048\n",
    "h = 8\n",
    "nwords = 8\n",
    "src_vocab_size = 25000\n",
    "tgt_vocab_size = 25000\n",
    "n_epochs = 10\n",
    "bs = 32\n",
    "pad_idx = 0\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# src_embed = torch.rand(bs, nwords, edim).to(device)\n",
    "# tgt_embed = torch.rand(bs, nwords, edim).to(device)\n",
    "# probs = transformer(src_embed, tgt_embed)\n",
    "# print(probs.shape)\n",
    "\n",
    "net = CustomTransformer(nx, edim, h, hdim, src_vocab_size, tgt_vocab_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), betas=[0.9, 0.98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'en_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000012?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):  \u001b[39m# loop over the dataset multiple times\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000012?line=3'>4</a>\u001b[0m     correct, total \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000012?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;00m src, src_mask, tgt, tgt_mask \u001b[39min\u001b[39;00m loader(en_tokens, de_tokens, bs, pad_idx):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000012?line=5'>6</a>\u001b[0m         ep\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000012?line=6'>7</a>\u001b[0m         src \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'en_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "ep = 0\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    for src, src_mask, tgt, tgt_mask in loader(en_tokens, de_tokens, bs, pad_idx):\n",
    "        ep+=1\n",
    "        src = src.to(device)\n",
    "        src_mask = src_mask.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        tgt_mask = tgt_mask.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(src, tgt, src_mask, tgt_mask)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(predicted.shape)\n",
    "        break\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if ep % num_steps == 0: \n",
    "#             val_accu = val(valloader)\n",
    "#             print(\"epoch {0} | loss: {1:.2f} | Val Accuracy: {2:.2f} %\".format(epoch, running_loss/num_steps, val_accu ))\n",
    "#             running_loss = 0.0\n",
    "#             if val_accu > best_accu:\n",
    "#               best_accu= val_accu \n",
    "#               torch.save(net.state_dict(), 'net_val.pth')\n",
    "#               print(\"Saving\")\n",
    "#             net.train()\n",
    "#     print(\"Train accuracy: {0:.2f} %\".format(100 * correct / total))\n",
    "\n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer_de_25000_start_token_SOS.json\")\n",
    "\n",
    "# for batch in loader_de.batch_iterator(1024):\n",
    "#     # print(batch)\n",
    "#     encoding = fast_tokenizer(batch)\n",
    "#     # decoded = fast_tokenizer.batch_decode(encoding['input_ids'])\n",
    "#     # print(decoded)\n",
    "#     # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('misc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1af58a0c390f807515830a18bbb19ac451fbe3aa00c4c733482807097ac6a02a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

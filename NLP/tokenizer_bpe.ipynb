{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import pickle\n",
    "from re import sub\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import time\n",
    "# from custom_transformer import CustomTransformer\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TextLoader:\n",
    "#     def __init__(self, PATH) -> None:\n",
    "        \n",
    "#         self.corpus = open(PATH, 'r').read()\n",
    "#         self.sentences = self.corpus.split('\\n')\n",
    "\n",
    "#     def remove_rare_words(self,vocab_dict, min_repeat):\n",
    "\n",
    "#         wordList = [k for k,v in vocab_dict.items() if v >= min_repeat]\n",
    "#         vocab = set(wordList)\n",
    "#         print(len(vocab), len(wordList))\n",
    "\n",
    "    \n",
    "#     def get_all_chars(self):\n",
    "#         chars = defaultdict(lambda: 0)\n",
    "#         for sen in self.sentences:\n",
    "#             for char in sen:\n",
    "#                 chars[char] += 1\n",
    "\n",
    "#         return chars\n",
    "\n",
    "#     def remove_sentence_with_rare_chars(self, min_count, dict):\n",
    "#         sens_idx = []\n",
    "#         for idx, sen in enumerate(self.sentences):\n",
    "#             flag = 1\n",
    "#             for char in sen:\n",
    "#                 if(dict[char] < min_count):\n",
    "#                     flag = 0\n",
    "#                     break\n",
    "#             if flag:\n",
    "#                 sens_idx.append(idx)\n",
    "#         return sens_idx\n",
    "\n",
    "#     def create_cleaned_data(self, idxs, ext):\n",
    "#         cleaned_sentences = [self.sentences[i] for i in idxs]\n",
    "#         self.sentences = cleaned_sentences\n",
    "#         print(len(self.sentences))\n",
    "\n",
    "#         clean_file = open('cleaned_train.' + ext, 'w+')\n",
    "#         for i in range(len(self.sentences)):  \n",
    "#             clean_file.write(self.sentences[i] + '\\n')\n",
    "            \n",
    "#     def batch_iterator(self, batch_size):\n",
    "#         for i in range(0, len(self.sentences), batch_size):\n",
    "#             yield self.sentences[i : i + batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader_en = TextLoader('datasets/train.en')\n",
    "# loader_de = TextLoader('datasets/train.de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars_en = loader_en.get_all_chars()\n",
    "# chars_de = loader_de.get_all_chars()\n",
    "\n",
    "# chars = defaultdict(lambda:0)\n",
    "# for k,v in chars_en.items():\n",
    "#     chars[k] += v\n",
    "# for k,v in chars_de.items():\n",
    "#     chars[k] += v\n",
    "\n",
    "# chars = dict(sorted(chars.items(), key=lambda item: item[1], reverse=True))\n",
    "# # dict(sorted(chars_de.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# cleaned_en = loader_en.remove_sentence_with_rare_chars(1000, chars)\n",
    "# cleaned_de = loader_de.remove_sentence_with_rare_chars(1000, chars)\n",
    "# cleaned_idx = list(set(cleaned_en) & set(cleaned_de))\n",
    "# len(cleaned_idx), len(cleaned_en), len(cleaned_de)\n",
    "\n",
    "# loader_de.create_cleaned_data(cleaned_idx, 'de')\n",
    "# loader_en.create_cleaned_data(cleaned_idx, 'en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader_en = TextLoader('datasets/cleaned_train.en')\n",
    "# loader_de = TextLoader('datasets/cleaned_train.de')\n",
    "\n",
    "# assert(len(loader_de.sentences) == len(loader_en.sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizer_en = Tokenizer(models.BPE())\n",
    "# tokenizer_en.normalizer = normalizers.Lowercase()\n",
    "# # tokenizer_en.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "# tokenizer_en.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "\n",
    "# trainer_en = trainers.BpeTrainer(vocab_size=25000,special_tokens=[\"[PAD]\"])\n",
    "# tokenizer_en.train_from_iterator(loader_en.batch_iterator(1000), trainer=trainer_en)\n",
    "\n",
    "# # tokenizer_en.decoder =  decoders.ByteLevel()\n",
    "# tokenizer_en.save(\"tokenizer_en_25000_start_token_SOS.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_en.decode([527, 1263, 197, 627, 48, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = tokenizer_en.encode(\"My name is Basu\")\n",
    "# print(encoding.tokens,encoding.ids )\n",
    "# tokenizer_en.decode(encoding.ids)\n",
    "\n",
    "# for batch in loader_en.batch_iterator(2):\n",
    "#     encoding = tokenizer_en.encode_batch(batch)\n",
    "#     print(tokenizer_en.decode_batch([encoding[i].ids for i in range(len(encoding))]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# li = []\n",
    "# for batch in loader_en.batch_iterator(1024):\n",
    "#     encoding = tokenizer_en.encode_batch(batch)\n",
    "#     for i in range(len(encoding)):\n",
    "#         li.append(len(encoding[i].ids))\n",
    "\n",
    "# li = sorted(li, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5000, 6000):\n",
    "#     if li[i] <= 100:\n",
    "#         print(i)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_de = Tokenizer(models.BPE())\n",
    "# tokenizer_de.normalizer = normalizers.Lowercase()\n",
    "# # tokenizer_de.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "# tokenizer_de.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "\n",
    "# # print(tokenizer.pre_tokenizer.pre_tokenize_str(\"Ich liebe das wirklich .\"))\n",
    "\n",
    "# trainer_de = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"[SOS]\",\"[EOS]\",\"[PAD]\"])\n",
    "# tokenizer_de.train_from_iterator(loader_de.batch_iterator(1000), trainer=trainer_de)\n",
    "\n",
    "# tokenizer_de.post_processor = processors.TemplateProcessing(\n",
    "#     single=f\"[SOS]:0 $A:0 [EOS]:0\",\n",
    "#     special_tokens=[(\"[SOS]\", 0), (\"[EOS]\", 1)])\n",
    "# # tokenizer_de.decoder =  decoders.ByteLevel()\n",
    "\n",
    "# tokenizer_de.save(\"tokenizer_de_25000_start_token_SOS.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_de.decode([i for i in range(300)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = tokenizer_de.encode(\"My name is Basu\")\n",
    "# print(encoding.tokens, encoding.ids)\n",
    "# tokenizer_de.decode(encoding.ids + [0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for batch in loader_de.batch_iterator(1024):\n",
    "#     encoding = tokenizer_de.encode_batch(batch)\n",
    "#     a = tokenizer_de.decode_batch([encoding[i].ids for i in range(len(encoding))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_tokens= []\n",
    "# encodes_len = []\n",
    "# for idx, batch in enumerate(loader_en.batch_iterator(1024)):\n",
    "#     encoding = tokenizer_en.encode_batch(batch)\n",
    "#     for i in range(len(encoding)):\n",
    "#         en_tokens.append(encoding[i].ids)\n",
    "#         encodes_len.append(len(encoding[i].ids))\n",
    "\n",
    "# de_tokens = []\n",
    "# for idx, batch in enumerate(loader_de.batch_iterator(1024)):\n",
    "#     encoding = tokenizer_de.encode_batch(batch)\n",
    "#     for i in range(len(encoding)):\n",
    "#         de_tokens.append(encoding[i].ids)\n",
    "\n",
    "# encodes_len , en_tokens, de_tokens = (list(t) for t in zip(*sorted(zip(encodes_len , en_encodes, de_encodes))))\n",
    "\n",
    "\n",
    "## Dump the tokenized lists as pickle files for faster retrieval\n",
    "# with open('datasets/en_tokenized.pkl', 'wb') as f:\n",
    "#     en_tokens = [np.array(i, dtype=np.uint16) for i in en_tokens]\n",
    "#     pickle.dump(en_tokens, f)\n",
    "\n",
    "# with open('datasets/de_tokenized.pkl', 'wb') as f:\n",
    "#     de_tokens = [np.array(i, dtype=np.uint16) for i in de_tokens]\n",
    "#     pickle.dump(de_tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4442492\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/en_tokenized.pkl', 'rb') as f:\n",
    "    en_tokens = pickle.load(f)\n",
    "\n",
    "with open('datasets/de_tokenized.pkl', 'rb') as f:\n",
    "    de_tokens = pickle.load(f)\n",
    "\n",
    "print(len(en_tokens))\n",
    "assert(len(de_tokens) == len(en_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_de = Tokenizer.from_file(\"tokenizer_de_25000_start_token_SOS.json\")\n",
    "tokenizer_en = Tokenizer.from_file(\"tokenizer_en_25000_start_token_SOS.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, edim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.edim = edim\n",
    "        self.dk = self.edim//self.h\n",
    "        self.key = nn.Linear(self.edim,self.edim)\n",
    "        self.query = nn.Linear(self.edim,self.edim)\n",
    "        self.value = nn.Linear(self.edim,self.edim)\n",
    "        self.linear = nn.Linear(self.edim,self.edim)\n",
    "        \n",
    "\n",
    "    def forward(self, key, value,query, mask = None):\n",
    "\n",
    "        bs = key.shape[0]\n",
    "        nwords_key = key.shape[1]\n",
    "        nwords_query = query.shape[1]\n",
    "\n",
    "        k = self.key(key).reshape(bs, nwords_key, self.h, self.dk).transpose(1,2)\n",
    "        q = self.query(query).reshape(bs, nwords_query, self.h, self.dk).transpose(1,2)\n",
    "        v = self.value(value).reshape(bs, nwords_key, self.h, self.dk).transpose(1,2)\n",
    "        x = torch.einsum('bhmd,bhnd -> bhmn',(q,k))\n",
    "        \n",
    "        if mask != None:\n",
    "            x = x.masked_fill(mask == False, float(\"-1e10\"))\n",
    "\n",
    "        x = F.softmax(x/(self.dk)**0.5, dim=3)\n",
    "\n",
    "        x = torch.einsum('bhmn,bhnv -> bhmv', (x,v))\n",
    "        x = x.transpose(1,2)\n",
    "\n",
    "        x = x.reshape(bs, nwords_query, -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, edim, h, hdim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multiHeadAttention = MultiHeadAttention(h, edim)\n",
    "        self.norm1 = nn.LayerNorm(edim)\n",
    "        self.norm2 = nn.LayerNorm(edim)\n",
    "        self.fc1 = nn.Linear(edim, hdim)\n",
    "        self.fc2 = nn.Linear(hdim, edim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, src_embed, src_mask):\n",
    "\n",
    "        x = self.multiHeadAttention(src_embed,src_embed,src_embed, src_mask)\n",
    "        x = self.dropout1(x)\n",
    "        subLayer1 = self.norm1(x + src_embed)\n",
    "\n",
    "        x = self.fc2(self.relu(self.fc1(subLayer1)))\n",
    "        x = self.dropout2(x)\n",
    "        subLayer2 = self.norm2(x + subLayer1)\n",
    "\n",
    "        return subLayer2\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,edim, h, hdim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multiHeadAttention = MultiHeadAttention(h, edim)\n",
    "        self.maskedMultiHeadAttention = MultiHeadAttention(h, edim)\n",
    "        self.norm1 = nn.LayerNorm(edim)\n",
    "        self.norm2 = nn.LayerNorm(edim)\n",
    "        self.norm3 = nn.LayerNorm(edim)\n",
    "        self.fc1 = nn.Linear(edim, hdim)\n",
    "        self.fc2 = nn.Linear(hdim, edim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt_embed, src_encoded, src_mask, tgt_mask):\n",
    "\n",
    "        x = self.maskedMultiHeadAttention(tgt_embed, tgt_embed, tgt_embed, tgt_mask)\n",
    "        x = self.dropout1(x)\n",
    "        subLayer1 = self.norm1(x + tgt_embed)\n",
    "\n",
    "        x = self.multiHeadAttention(src_encoded, src_encoded, subLayer1, src_mask)\n",
    "        x = self.dropout2(x)\n",
    "        subLayer2 = self.norm2(x + subLayer1)\n",
    "\n",
    "        x = self.fc2(self.relu(self.fc1(subLayer2)))\n",
    "        x = self.dropout3(x)\n",
    "        subLayer3 = self.norm3(x + subLayer2)\n",
    "        \n",
    "        return subLayer3\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, nx, edim, h, hdim,dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nx = nx\n",
    "        self.transformers = nn.ModuleList([EncoderBlock(edim, h, hdim,dropout) for _ in range(nx)])\n",
    "\n",
    "    def forward(self, src_embed, src_mask):\n",
    "        for block in self.transformers:\n",
    "            embed = block(src_embed, src_mask)\n",
    "        return embed\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, nx, edim, h, hdim,dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nx = nx\n",
    "        self.transformers = nn.ModuleList([DecoderBlock(edim, h, hdim,dropout) for _ in range(nx)])\n",
    "\n",
    "    def forward(self, encoded, tgt_embed, src_mask, tgt_mask):\n",
    "\n",
    "        for block in self.transformers:\n",
    "            embed = block(tgt_embed, encoded, src_mask, tgt_mask)\n",
    "        return embed\n",
    "\n",
    "\n",
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self, nx, edim, h, hdim, dropout, src_vocab_size, tgt_vocab_size):\n",
    "        super().__init__()\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size,edim)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size,edim)\n",
    "        self.encoder = Encoder(nx, edim, h, hdim,dropout)\n",
    "        self.decoder = Decoder(nx, edim, h, hdim,dropout)\n",
    "        self.fc = nn.Linear(edim, tgt_vocab_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src_tokens, tgt_tokens, src_mask, tgt_mask, src_pos_embed, tgt_pos_embed, encoded = None):\n",
    "        \n",
    "        if encoded == None:\n",
    "\n",
    "            src_embed = self.src_embedding(src_tokens) + src_pos_embed\n",
    "            src_embed = self.dropout1(src_embed)\n",
    "            encoded = self.encoder(src_embed, src_mask)\n",
    "\n",
    "        tgt_embed = self.tgt_embedding(tgt_tokens) + tgt_pos_embed\n",
    "        tgt_embed = self.dropout2(tgt_embed)\n",
    "        \n",
    "        decoded = self.decoder(encoded, tgt_embed, src_mask, tgt_mask)\n",
    "        output = self.fc(decoded)\n",
    "\n",
    "        return output, encoded\n",
    "\n",
    "    # def forward(self, src_tokens, tgt_tokens, src_mask, tgt_mask, src_pos_embed, tgt_pos_embed):\n",
    "\n",
    "    #     src_embed = self.src_embedding(src_tokens) + src_pos_embed\n",
    "    #     src_embed = self.dropout1(src_embed)\n",
    "    #     if torch.isnan(src_embed).any():\n",
    "    #         print(\"EMBEDING IS WRONG\")\n",
    "    #         print(src_embed)\n",
    "    #     tgt_embed = self.tgt_embedding(tgt_tokens) + tgt_pos_embed\n",
    "    #     tgt_embed = self.dropout2(tgt_embed)\n",
    "    #     if torch.isnan(tgt_embed).any():\n",
    "    #         print(\"TGT IS WRONG\")\n",
    "    #         print(tgt_embed)\n",
    "    #     encoded = self.encoder(src_embed, src_mask)\n",
    "    #     if torch.isnan(encoded).any():\n",
    "    #         print(\"ENCODER IS WRONG\")\n",
    "    #         print(encoded)\n",
    "    #     decoded = self.decoder(encoded, tgt_embed, src_mask, tgt_mask)\n",
    "    #     if torch.isnan(encoded).any():\n",
    "    #         print(\"DECODER IS WRONG\")\n",
    "    #         print(encoded)\n",
    "    #     output = self.fc(decoded)\n",
    "\n",
    "    #     return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(en_tokens, de_tokens, bs, src_pad_idx, tgt_pad_idx, shuffle = True):\n",
    "    num_batches = len(en_tokens)//bs\n",
    "    idxs = [i for i in range(num_batches)]\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(idxs)\n",
    "\n",
    "    for i in idxs:\n",
    "        max_len = len(en_tokens[(i+1)*bs - 1])\n",
    "        en_tensor = torch.tensor([enc.tolist() + \n",
    "        [src_pad_idx]*(max_len - len(enc)) for enc in en_tokens[i*bs:(i+1)*bs]])\n",
    "        \n",
    "        en_pad = torch.tensor((en_tensor != src_pad_idx)).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        de_lens = [len(enc) for enc in de_tokens[i*bs:(i+1)*bs]]\n",
    "        max_len= max(de_lens)\n",
    "        de_tensor = torch.tensor([enc.tolist() + \n",
    "        [tgt_pad_idx]*(max_len - len(enc)) for enc in de_tokens[i*bs:(i+1)*bs]])\n",
    "        \n",
    "\n",
    "        de_mask = torch.ones(bs, max_len, max_len).tril().unsqueeze(1)\n",
    "\n",
    "\n",
    "        yield en_tensor, en_pad, de_tensor, de_mask, de_lens\n",
    "\n",
    "\n",
    "def positionEmbeding(edim, max_nwords):\n",
    "    pos_emb = torch.zeros((max_nwords, edim))\n",
    "    for pos in range(max_nwords):\n",
    "        for i in range(edim//2):\n",
    "            pos_emb[pos, 2*i] = np.sin(pos/(10000**(2*i/edim)))\n",
    "            pos_emb[pos, 2*i + 1] = np.cos(pos/(10000**(2*i/edim)))\n",
    "\n",
    "    return pos_emb\n",
    "\n",
    "def remove_large_Sentences(en_tokens, de_tokens, max_size):\n",
    "    li = []\n",
    "    for i in range(len(en_tokens)):\n",
    "        if(len(en_tokens[i]) <= max_size and len(de_tokens[i]) <= max_size):\n",
    "            li.append(i)\n",
    "\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "\n",
    "    def __init__(self, model_size, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self):\n",
    "        return self.model_size ** (-0.5) *min(self._step ** (-0.5),  self._step * self.warmup ** (-1.5))\n",
    "\n",
    "def save_model(PATH):\n",
    "    state = {\n",
    "      'state_dict': net.state_dict(),\n",
    "      'optimizer': optimizer.state_dict(),\n",
    "      'scheduler_step': scheduler._step\n",
    "      }\n",
    "    torch.save(state, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "nx = 6\n",
    "edim = 512\n",
    "hdim = 2048\n",
    "h = 8\n",
    "src_vocab_size = 25000\n",
    "tgt_vocab_size = 25000\n",
    "n_epochs = 10\n",
    "bs = 32\n",
    "src_pad_idx = 0\n",
    "tgt_pad_idx = 2\n",
    "dropout = 0.1\n",
    "max_nwords = 100\n",
    "n_epochs = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "net = CustomTransformer(nx, edim, h, hdim, dropout, src_vocab_size, tgt_vocab_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), betas=[0.9, 0.98])\n",
    "scheduler = NoamOpt(edim,2500,optimizer)\n",
    "# scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"saved_models/n_steps_65000\"\n",
    "state = torch.load(PATH)\n",
    "net.load_state_dict(state['state_dict'])\n",
    "optimizer.load_state_dict(state['optimizer'])\n",
    "scheduler._step = state['scheduler_step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idxs = remove_large_Sentences(en_tokens, de_tokens, max_nwords)\n",
    "en_tokens = [en_tokens[i] for i in max_idxs]\n",
    "de_tokens = [de_tokens[i] for i in max_idxs]\n",
    "assert(len(de_tokens) == len(en_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbasujindal\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/thunder/my_github_clones/CodeSnippets/NLP/wandb/run-20220629_195534-1osbe6xo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/basujindal/transformers/runs/1osbe6xo\" target=\"_blank\">worldly-vortex-31</a></strong> to <a href=\"https://wandb.ai/basujindal/transformers\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"transformers\")\n",
    "wandb.config = {\n",
    "  \"nsteps\": 500000,\n",
    "  \"batch_size\": bs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 68000\n",
    "bs = 32\n",
    "\n",
    "net.train()\n",
    "posEmb = positionEmbeding(edim, max_nwords)\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    p_bar=tqdm(total=len(en_tokens)//bs)\n",
    "  \n",
    "    for src, src_mask, tgt, tgt_mask, tgt_lens in loader(en_tokens, de_tokens, bs, src_pad_idx, tgt_pad_idx):\n",
    "        p_bar.update(1)\n",
    "\n",
    "\n",
    "        labels = torch.cat([tgt[ii][1:tgt_lens[ii]] for ii in range(tgt.shape[0])], dim = 0).to(device)\n",
    "        step+=1\n",
    "        src = src.to(device)\n",
    "        src_mask = src_mask.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        tgt_mask = tgt_mask.to(device)\n",
    "\n",
    "\n",
    "        src_pos_embed = posEmb[:src.shape[1]].unsqueeze(0).to(device)\n",
    "        tgt_pos_embed = posEmb[:tgt.shape[1]].unsqueeze(0).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # with torch.autograd.detect_anomaly():\n",
    "        # if 1:\n",
    "        #     with torch.cuda.amp.autocast():\n",
    "        outputs, _ = net(src, tgt, src_mask, tgt_mask, src_pos_embed, tgt_pos_embed)\n",
    "\n",
    "        li = [outputs[ii][:tgt_lens[ii]-1] for ii in range(outputs.shape[0])]\n",
    "        probs = torch.cat(li, dim = 0)\n",
    "        loss = criterion(probs, labels)   \n",
    "\n",
    "        if(step%100 == 0 and  np.isnan(loss.data.cpu().numpy())):\n",
    "            print(\"Loss is NaN\")\n",
    "            print(outputs)\n",
    "            break\n",
    "\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        predicted = torch.max(probs, 1)[1]\n",
    "        \n",
    "        total = labels.size(0)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "\n",
    "        if step%2:\n",
    "            scheduler.step()\n",
    "\n",
    "        try:\n",
    "            wandb.log(\n",
    "                {\"loss\": loss.data,\n",
    "                \"lr\": scheduler._rate,\n",
    "                \"accuracy\": correct/total,\n",
    "                })\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if(step%5000 == 0):\n",
    "            PATH = \"saved_models/n_steps_\" + str(step)\n",
    "            save_model(PATH)\n",
    "\n",
    "\n",
    "        # Optional\n",
    "        # wandb.watch(net)\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if ep % num_steps == 0: \n",
    "#             val_accu = val(valloader)\n",
    "#             print(\"epoch {0} | loss: {1:.2f} | Val Accuracy: {2:.2f} %\".format(epoch, running_loss/num_steps, val_accu ))\n",
    "#             running_loss = 0.0\n",
    "#             if val_accu > best_accu:\n",
    "#               best_accu= val_accu \n",
    "#               torch.save(net.state_dict(), 'net_val.pth')\n",
    "#               print(\"Saving\")\n",
    "#             net.train()\n",
    "#     print(\"Train accuracy: {0:.2f} %\".format(100 * correct / total))\n",
    "\n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, net, device):\n",
    "\n",
    "    net.eval()\n",
    "    posEmb = positionEmbeding(edim, max_nwords)\n",
    "\n",
    "    src_enc = tokenizer_en.encode(sentence).ids\n",
    "    src = torch.tensor(src_enc).unsqueeze(0)\n",
    "    src_mask = None\n",
    "\n",
    "    tgt_enc = [0]\n",
    "    tgt = torch.tensor(tgt_enc).unsqueeze(0)\n",
    "    tgt_mask =  torch.ones(1, len(tgt), len(tgt)).tril().unsqueeze(1).to(device)\n",
    "\n",
    "    src = src.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "\n",
    "    src_pos_embed = posEmb[:src.shape[1]].unsqueeze(0).to(device)\n",
    "    tgt_pos_embed = posEmb[:tgt.shape[1]].unsqueeze(0).to(device)\n",
    "\n",
    "    output, encoded = net(src, tgt, src_mask, tgt_mask, src_pos_embed, tgt_pos_embed)\n",
    "\n",
    "    _, predicted_idx = torch.max(output.data[0], 1)\n",
    "    print(tgt)\n",
    "    print(predicted_idx.tolist())\n",
    "    print(tokenizer_de.decode(predicted_idx.tolist()))\n",
    "\n",
    "\n",
    "    idx = 0\n",
    "    while(predicted_idx[-1].tolist() != 1):\n",
    "        \n",
    "        tgt = torch.cat((tgt, predicted_idx[-1].unsqueeze(0).unsqueeze(0)), 1)\n",
    "        tgt_mask =  torch.ones(1, tgt.shape[-1], tgt.shape[-1]).tril().unsqueeze(1).to(device)\n",
    "        # print(tgt)\n",
    "        output, _ = net(src, tgt, src_mask, tgt_mask, src_pos_embed, tgt_pos_embed, encoded)\n",
    "        _, predicted_idx = torch.max(output.data[0], 1)\n",
    "        print(predicted_idx.tolist())\n",
    "        print(tokenizer_de.decode(predicted_idx.tolist()))\n",
    "        idx+=1\n",
    "\n",
    "        if(idx == max_nwords):\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx = 6\n",
    "edim = 512\n",
    "hdim = 2048\n",
    "h = 8\n",
    "src_vocab_size = 25000\n",
    "tgt_vocab_size = 25000\n",
    "pad_idx = 0\n",
    "dropout = 0.1\n",
    "max_nwords = 100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "net = CustomTransformer(nx, edim, h, hdim, dropout, src_vocab_size, tgt_vocab_size).to(device)\n",
    "\n",
    "PATH = \"saved_models/n_steps_65000\"\n",
    "state = torch.load(PATH)\n",
    "net.load_state_dict(state['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0]], device='cuda:0')\n",
      "[310]\n",
      "wie\n",
      "[310, 442]\n",
      "wie können\n",
      "[310, 442, 264]\n",
      "wie können sie\n",
      "[310, 442, 264, 310]\n",
      "wie können sie wie\n",
      "[310, 442, 264, 310, 442]\n",
      "wie können sie wie können\n",
      "[310, 442, 264, 310, 442, 28]\n",
      "wie können sie wie können ?\n",
      "[310, 442, 264, 310, 442, 28, 1]\n",
      "wie können sie wie können ?\n"
     ]
    }
   ],
   "source": [
    "sentence = 'How are you?'\n",
    "translate(sentence, net, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('misc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1af58a0c390f807515830a18bbb19ac451fbe3aa00c4c733482807097ac6a02a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbasujindal\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/thunder/my_github_clones/CodeSnippets/NLP/wandb/run-20220627_165939-3g4oilwe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/basujindal/transformers/runs/3g4oilwe\" target=\"_blank\">misunderstood-snow-2</a></strong> to <a href=\"https://wandb.ai/basujindal/transformers\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/basujindal/transformers/runs/3g4oilwe?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fe4be4f4760>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import pickle\n",
    "from re import sub\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import time\n",
    "# from custom_transformer import CustomTransformer\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"transformers\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TextLoader:\n",
    "#     def __init__(self, PATH) -> None:\n",
    "        \n",
    "#         self.corpus = open(PATH, 'r').read()\n",
    "#         self.sentences = self.corpus.split('\\n')\n",
    "\n",
    "#     def remove_rare_words(self,vocab_dict, min_repeat):\n",
    "\n",
    "#         wordList = [k for k,v in vocab_dict.items() if v >= min_repeat]\n",
    "#         vocab = set(wordList)\n",
    "#         print(len(vocab), len(wordList))\n",
    "\n",
    "    \n",
    "#     def get_all_chars(self):\n",
    "#         chars = defaultdict(lambda: 0)\n",
    "#         for sen in self.sentences:\n",
    "#             for char in sen:\n",
    "#                 chars[char] += 1\n",
    "\n",
    "#         return chars\n",
    "\n",
    "#     def remove_sentence_with_rare_chars(self, min_count, dict):\n",
    "#         sens_idx = []\n",
    "#         for idx, sen in enumerate(self.sentences):\n",
    "#             flag = 1\n",
    "#             for char in sen:\n",
    "#                 if(dict[char] < min_count):\n",
    "#                     flag = 0\n",
    "#                     break\n",
    "#             if flag:\n",
    "#                 sens_idx.append(idx)\n",
    "#         return sens_idx\n",
    "\n",
    "#     def create_cleaned_data(self, idxs, ext):\n",
    "#         cleaned_sentences = [self.sentences[i] for i in idxs]\n",
    "#         self.sentences = cleaned_sentences\n",
    "#         print(len(self.sentences))\n",
    "\n",
    "#         clean_file = open('cleaned_train.' + ext, 'w+')\n",
    "#         for i in range(len(self.sentences)):  \n",
    "#             clean_file.write(self.sentences[i] + '\\n')\n",
    "            \n",
    "#     def batch_iterator(self, batch_size):\n",
    "#         for i in range(0, len(self.sentences), batch_size):\n",
    "#             yield self.sentences[i : i + batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader_en = TextLoader('datasets/train.en')\n",
    "# loader_de = TextLoader('datasets/train.de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars_en = loader_en.get_all_chars()\n",
    "# chars_de = loader_de.get_all_chars()\n",
    "\n",
    "# chars = defaultdict(lambda:0)\n",
    "# for k,v in chars_en.items():\n",
    "#     chars[k] += v\n",
    "# for k,v in chars_de.items():\n",
    "#     chars[k] += v\n",
    "\n",
    "# chars = dict(sorted(chars.items(), key=lambda item: item[1], reverse=True))\n",
    "# # dict(sorted(chars_de.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# cleaned_en = loader_en.remove_sentence_with_rare_chars(1000, chars)\n",
    "# cleaned_de = loader_de.remove_sentence_with_rare_chars(1000, chars)\n",
    "# cleaned_idx = list(set(cleaned_en) & set(cleaned_de))\n",
    "# len(cleaned_idx), len(cleaned_en), len(cleaned_de)\n",
    "\n",
    "# loader_de.create_cleaned_data(cleaned_idx, 'de')\n",
    "# loader_en.create_cleaned_data(cleaned_idx, 'en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader_en = TextLoader('datasets/cleaned_train.en')\n",
    "# loader_de = TextLoader('datasets/cleaned_train.de')\n",
    "\n",
    "# assert(len(loader_de.sentences) == len(loader_en.sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizer_en = Tokenizer(models.BPE())\n",
    "# tokenizer_en.normalizer = normalizers.Lowercase()\n",
    "# # tokenizer_en.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "# tokenizer_en.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "\n",
    "# trainer_en = trainers.BpeTrainer(vocab_size=25000,special_tokens=[\"[PAD]\"])\n",
    "# tokenizer_en.train_from_iterator(loader_en.batch_iterator(1000), trainer=trainer_en)\n",
    "\n",
    "# # tokenizer_en.decoder =  decoders.ByteLevel()\n",
    "# tokenizer_en.save(\"tokenizer_en_25000_start_token_SOS.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_en.decode([527, 1263, 197, 627, 48, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = tokenizer_en.encode(\"My name is Basu\")\n",
    "# print(encoding.tokens,encoding.ids )\n",
    "# tokenizer_en.decode(encoding.ids)\n",
    "\n",
    "# for batch in loader_en.batch_iterator(2):\n",
    "#     encoding = tokenizer_en.encode_batch(batch)\n",
    "#     print(tokenizer_en.decode_batch([encoding[i].ids for i in range(len(encoding))]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# li = []\n",
    "# for batch in loader_en.batch_iterator(1024):\n",
    "#     encoding = tokenizer_en.encode_batch(batch)\n",
    "#     for i in range(len(encoding)):\n",
    "#         li.append(len(encoding[i].ids))\n",
    "\n",
    "# li = sorted(li, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5000, 6000):\n",
    "#     if li[i] <= 100:\n",
    "#         print(i)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_de = Tokenizer(models.BPE())\n",
    "# tokenizer_de.normalizer = normalizers.Lowercase()\n",
    "# # tokenizer_de.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "# tokenizer_de.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "\n",
    "# # print(tokenizer.pre_tokenizer.pre_tokenize_str(\"Ich liebe das wirklich .\"))\n",
    "\n",
    "# trainer_de = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"[PAD]\", \"[SOS]\", \"[EOS]\"])\n",
    "# tokenizer_de.train_from_iterator(loader_de.batch_iterator(1000), trainer=trainer_de)\n",
    "\n",
    "# tokenizer_de.post_processor = processors.TemplateProcessing(\n",
    "#     single=f\"[SOS]:0 $A:0 [EOS]:0\",\n",
    "#     special_tokens=[(\"[SOS]\", 0), (\"[EOS]\", 1)])\n",
    "# # tokenizer_de.decoder =  decoders.ByteLevel()\n",
    "\n",
    "# tokenizer_de.save(\"tokenizer_de_25000_start_token_SOS.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_de.decode([i for i in range(300)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = tokenizer_de.encode(\"My name is Basu\")\n",
    "# print(encoding.tokens, encoding.ids)\n",
    "# tokenizer_de.decode(encoding.ids + [0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for batch in loader_de.batch_iterator(1024):\n",
    "#     encoding = tokenizer_de.encode_batch(batch)\n",
    "#     a = tokenizer_de.decode_batch([encoding[i].ids for i in range(len(encoding))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_encodes = []\n",
    "# encodes_len = []\n",
    "# for idx, batch in enumerate(loader_en.batch_iterator(1024)):\n",
    "#     encoding = tokenizer_en.encode_batch(batch)\n",
    "#     for i in range(len(encoding)):\n",
    "#         en_encodes.append(encoding[i].ids)\n",
    "#         encodes_len.append(len(encoding[i].ids))\n",
    "\n",
    "# de_encodes = []\n",
    "# for idx, batch in enumerate(loader_de.batch_iterator(1024)):\n",
    "#     encoding = tokenizer_de.encode_batch(batch)\n",
    "#     for i in range(len(encoding)):\n",
    "#         de_encodes.append(encoding[i].ids)\n",
    "\n",
    "# encodes_len , en_encodes, de_encodes = (list(t) for t in zip(*sorted(zip(encodes_len , en_encodes, de_encodes))))\n",
    "\n",
    "\n",
    "# # Dump the tokenized lists as pickle files for faster retrieval\n",
    "# with open('datasets/en_tokenized.pkl', 'wb') as f:\n",
    "#     pickle.dump(en_encodes, f)\n",
    "\n",
    "# with open('datasets/de_tokenized.pkl', 'wb') as f:\n",
    "#     pickle.dump(de_encodes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4442492\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/en_tokenized.pkl', 'rb') as f:\n",
    "    en_tokens = pickle.load(f)\n",
    "\n",
    "with open('datasets/de_tokenized.pkl', 'rb') as f:\n",
    "    de_tokens = pickle.load(f)\n",
    "\n",
    "print(len(en_tokens))\n",
    "assert(len(de_tokens) == len(en_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_large_Sentences(en_tokens, de_tokens, max_size):\n",
    "    li = []\n",
    "    for i in range(len(en_tokens)):\n",
    "        if(len(en_tokens[i]) <= max_size and len(de_tokens[i]) <= max_size):\n",
    "            li.append(i)\n",
    "\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_de = Tokenizer.from_file(\"tokenizer_de_25000_start_token_SOS.json\")\n",
    "tokenizer_en = Tokenizer.from_file(\"tokenizer_en_25000_start_token_SOS.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, edim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.edim = edim\n",
    "        self.dk = self.edim//self.h\n",
    "        self.key = nn.Linear(self.edim,self.edim)\n",
    "        self.query = nn.Linear(self.edim,self.edim)\n",
    "        self.value = nn.Linear(self.edim,self.edim)\n",
    "        self.linear = nn.Linear(self.edim,self.edim)\n",
    "        \n",
    "\n",
    "    def forward(self, key, value,query, mask = None):\n",
    "\n",
    "        bs = key.shape[0]\n",
    "        nwords_key = key.shape[1]\n",
    "        nwords_query = query.shape[1]\n",
    "\n",
    "        k = self.key(key).reshape(bs, nwords_key, self.h, self.dk).transpose(1,2)\n",
    "        q = self.query(query).reshape(bs, nwords_query, self.h, self.dk).transpose(1,2)\n",
    "        v = self.value(value).reshape(bs, nwords_key, self.h, self.dk).transpose(1,2)\n",
    "        x = torch.einsum('bhmd,bhnd -> bhmn',(q,k))\n",
    "        \n",
    "        if mask != None:\n",
    "            x = x.masked_fill(mask == False, float(\"-1e20\"))\n",
    "\n",
    "        x = F.softmax(x/(self.dk)**0.5, dim=3)\n",
    "\n",
    "        x = torch.einsum('bhmn,bhnv -> bhmv', (x,v))\n",
    "        x = x.transpose(1,2)\n",
    "\n",
    "        x = x.reshape(bs, nwords_query, -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, edim, h, hdim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multiHeadAttention = MultiHeadAttention(h, edim)\n",
    "        self.norm1 = nn.LayerNorm(edim)\n",
    "        self.norm2 = nn.LayerNorm(edim)\n",
    "        self.fc1 = nn.Linear(edim, hdim)\n",
    "        self.fc2 = nn.Linear(hdim, edim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, src_embed, src_mask):\n",
    "\n",
    "        x = self.multiHeadAttention(src_embed,src_embed,src_embed, src_mask)\n",
    "        x = self.dropout1(x)\n",
    "        subLayer1 = self.norm1(x + src_embed)\n",
    "\n",
    "        x = self.fc2(self.relu(self.fc1(subLayer1)))\n",
    "        x = self.dropout2(x)\n",
    "        subLayer2 = self.norm2(x + subLayer1)\n",
    "\n",
    "        return subLayer2\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,edim, h, hdim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multiHeadAttention = MultiHeadAttention(h, edim)\n",
    "        self.maskedMultiHeadAttention = MultiHeadAttention(h, edim)\n",
    "        self.norm1 = nn.LayerNorm(edim)\n",
    "        self.norm2 = nn.LayerNorm(edim)\n",
    "        self.norm3 = nn.LayerNorm(edim)\n",
    "        self.fc1 = nn.Linear(edim, hdim)\n",
    "        self.fc2 = nn.Linear(hdim, edim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt_embed, src_encoded, src_mask, tgt_mask):\n",
    "\n",
    "        x = self.maskedMultiHeadAttention(tgt_embed, tgt_embed, tgt_embed, tgt_mask)\n",
    "        x = self.dropout1(x)\n",
    "        subLayer1 = self.norm1(x + tgt_embed)\n",
    "\n",
    "        x = self.multiHeadAttention(src_encoded, src_encoded, subLayer1, src_mask)\n",
    "        x = self.dropout2(x)\n",
    "        subLayer2 = self.norm2(x + subLayer1)\n",
    "\n",
    "        x = self.fc2(self.relu(self.fc1(subLayer2)))\n",
    "        x = self.dropout3(x)\n",
    "        subLayer3 = self.norm3(x + subLayer2)\n",
    "        \n",
    "        return subLayer3\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, nx, edim, h, hdim,dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nx = nx\n",
    "        self.transformers = nn.ModuleList([EncoderBlock(edim, h, hdim,dropout) for _ in range(nx)])\n",
    "\n",
    "    def forward(self, src_embed, src_mask):\n",
    "        for block in self.transformers:\n",
    "            embed = block(src_embed, src_mask)\n",
    "        return embed\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, nx, edim, h, hdim,dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nx = nx\n",
    "        self.transformers = nn.ModuleList([DecoderBlock(edim, h, hdim,dropout) for _ in range(nx)])\n",
    "\n",
    "    def forward(self, encoded, tgt_embed, src_mask, tgt_mask):\n",
    "\n",
    "        for block in self.transformers:\n",
    "            embed = block(tgt_embed, encoded, src_mask, tgt_mask)\n",
    "        return embed\n",
    "\n",
    "\n",
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self, nx, edim, h, hdim, dropout, src_vocab_size, tgt_vocab_size):\n",
    "        super().__init__()\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size,edim)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size,edim)\n",
    "        self.encoder = Encoder(nx, edim, h, hdim,dropout)\n",
    "        self.decoder = Decoder(nx, edim, h, hdim,dropout)\n",
    "        self.fc = nn.Linear(edim, tgt_vocab_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src_tokens, tgt_tokens, src_mask, tgt_mask, src_pos_embed, tgt_pos_embed):\n",
    "\n",
    "        src_embed = self.src_embedding(src_tokens) + src_pos_embed\n",
    "        src_embed = self.dropout1(src_embed)\n",
    "        tgt_embed = self.tgt_embedding(tgt_tokens) + tgt_pos_embed\n",
    "        tgt_embed = self.dropout2(tgt_embed)\n",
    "        encoded = self.encoder(src_embed, src_mask)\n",
    "        decoded = self.decoder(encoded, tgt_embed, src_mask, tgt_mask)\n",
    "        output = self.fc(decoded)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(en_tokens, de_tokens, bs, pad_idx, shuffle = True):\n",
    "    num_batches = len(en_tokens)//bs\n",
    "    idxs = [i for i in range(num_batches)]\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(idxs)\n",
    "\n",
    "    for i in idxs:\n",
    "        max_len = len(en_tokens[(i+1)*bs - 1])\n",
    "        en_tensor = torch.tensor([enc + \n",
    "        [pad_idx]*(max_len - len(enc)) for enc in en_tokens[i*bs:(i+1)*bs]])\n",
    "        \n",
    "        en_pad = torch.tensor((en_tensor != pad_idx)).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        de_lens = [len(enc) for enc in de_tokens[i*bs:(i+1)*bs]]\n",
    "        max_len= max(de_lens)\n",
    "        de_tensor = torch.tensor([enc + \n",
    "        [pad_idx]*(max_len - len(enc)) for enc in de_tokens[i*bs:(i+1)*bs]])\n",
    "        \n",
    "\n",
    "        de_mask = torch.ones(bs, max_len, max_len).tril().unsqueeze(1)\n",
    "\n",
    "\n",
    "        yield en_tensor, en_pad, de_tensor, de_mask, de_lens\n",
    "\n",
    "\n",
    "def positionEmbeding(edim, max_nwords):\n",
    "    pos_emb = torch.zeros((max_nwords, edim))\n",
    "    for pos in range(max_nwords):\n",
    "        for i in range(edim//2):\n",
    "            pos_emb[pos, 2*i] = np.sin(pos/(10000**(2*i/edim)))\n",
    "            pos_emb[pos, 2*i + 1] = np.cos(pos/(10000**(2*i/edim)))\n",
    "\n",
    "    return pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "\n",
    "    def __init__(self, model_size, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self):\n",
    "        return self.model_size ** (-0.5) *min(self._step ** (-0.5),  self._step * self.warmup ** (-1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "nx = 6\n",
    "edim = 512\n",
    "hdim = 2048\n",
    "h = 8\n",
    "src_vocab_size = 25000\n",
    "tgt_vocab_size = 25000\n",
    "n_epochs = 10\n",
    "bs = 32\n",
    "pad_idx = 0\n",
    "dropout = 0.1\n",
    "max_nwords = 100\n",
    "\n",
    "max_idxs = remove_large_Sentences(en_tokens, de_tokens, max_nwords)\n",
    "en_tokens = [en_tokens[i] for i in max_idxs]\n",
    "de_tokens = [de_tokens[i] for i in max_idxs]\n",
    "assert(len(de_tokens) == len(en_tokens))\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "net = CustomTransformer(nx, edim, h, hdim, dropout, src_vocab_size, tgt_vocab_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), betas=[0.9, 0.98])\n",
    "scheduler = NoamOpt(edim,4000,optimizer)\n",
    "\n",
    "wandb.config = {\n",
    "  \"nsteps\": 500000,\n",
    "  \"batch_size\": bs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Percentage :   0%|          | 0/138394 [00:00<?, ?it/s]/tmp/ipykernel_54505/2227623443.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  en_pad = torch.tensor((en_tensor != pad_idx)).unsqueeze(1).unsqueeze(2)\n",
      "Percentage :  11%|█         | 15358/138394 [18:43<2:08:46, 15.92it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb Cell 23'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000022?line=19'>20</a>\u001b[0m tgt_pos_embed \u001b[39m=\u001b[39m posEmb[:tgt\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000022?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000022?line=22'>23</a>\u001b[0m outputs \u001b[39m=\u001b[39m net(src, tgt, src_mask, tgt_mask, src_pos_embed, tgt_pos_embed)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000022?line=23'>24</a>\u001b[0m \u001b[39m# print(outputs.shape[0])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000022?line=25'>26</a>\u001b[0m li \u001b[39m=\u001b[39m [outputs[ii][:tgt_lens[ii]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m ii \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(outputs\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])]\n",
      "File \u001b[0;32m~/anaconda3/envs/misc/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb Cell 19'\u001b[0m in \u001b[0;36mCustomTransformer.forward\u001b[0;34m(self, src_tokens, tgt_tokens, src_mask, tgt_mask, src_pos_embed, tgt_pos_embed)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=138'>139</a>\u001b[0m tgt_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtgt_embedding(tgt_tokens) \u001b[39m+\u001b[39m tgt_pos_embed\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=139'>140</a>\u001b[0m tgt_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2(tgt_embed)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=140'>141</a>\u001b[0m encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src_embed, src_mask)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=141'>142</a>\u001b[0m decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(encoded, tgt_embed, src_mask, tgt_mask)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=142'>143</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(decoded)\n",
      "File \u001b[0;32m~/anaconda3/envs/misc/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb Cell 19'\u001b[0m in \u001b[0;36mEncoder.forward\u001b[0;34m(self, src_embed, src_mask)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=103'>104</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src_embed, src_mask):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=104'>105</a>\u001b[0m     \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformers:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=105'>106</a>\u001b[0m         embed \u001b[39m=\u001b[39m block(src_embed, src_mask)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=106'>107</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m embed\n",
      "File \u001b[0;32m~/anaconda3/envs/misc/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb Cell 19'\u001b[0m in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, src_embed, src_mask)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=51'>52</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src_embed, src_mask):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=53'>54</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultiHeadAttention(src_embed,src_embed,src_embed, src_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=54'>55</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=55'>56</a>\u001b[0m     subLayer1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m src_embed)\n",
      "File \u001b[0;32m~/anaconda3/envs/misc/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb Cell 19'\u001b[0m in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, key, value, query, mask)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=16'>17</a>\u001b[0m nwords_key \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=17'>18</a>\u001b[0m nwords_query \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=19'>20</a>\u001b[0m k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey(key)\u001b[39m.\u001b[39mreshape(bs, nwords_key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdk)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=20'>21</a>\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery(query)\u001b[39m.\u001b[39mreshape(bs, nwords_query, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdk)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/thunder/my_github_clones/CodeSnippets/NLP/tokenizer_bpe.ipynb#ch0000018?line=21'>22</a>\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(value)\u001b[39m.\u001b[39mreshape(bs, nwords_key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdk)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/misc/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Percentage :  11%|█         | 15359/138394 [18:54<2:08:46, 15.92it/s]"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "ep = 0\n",
    "\n",
    "posEmb = positionEmbeding(edim, max_nwords)\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    p_bar=tqdm(total=len(en_tokens)//bs,desc=\"Percentage \")\n",
    "  \n",
    "    for src, src_mask, tgt, tgt_mask, tgt_lens in loader(en_tokens, de_tokens, bs, pad_idx):\n",
    "        p_bar.update(1)\n",
    "\n",
    "\n",
    "        labels = torch.cat([tgt[ii][1:tgt_lens[ii]] for ii in range(tgt.shape[0])], dim = 0)\n",
    "        ep+=1\n",
    "        src = src.to(device)\n",
    "        src_mask = src_mask.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        tgt_mask = tgt_mask.to(device)\n",
    "\n",
    "\n",
    "        src_pos_embed = posEmb[:src.shape[1]].unsqueeze(0).to(device)\n",
    "        tgt_pos_embed = posEmb[:tgt.shape[1]].unsqueeze(0).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(src, tgt, src_mask, tgt_mask, src_pos_embed, tgt_pos_embed)\n",
    "        # print(outputs.shape[0])\n",
    "\n",
    "        li = [outputs[ii][:tgt_lens[ii]-1] for ii in range(outputs.shape[0])]\n",
    "        probs = torch.cat(li, dim = 0)\n",
    "\n",
    "\n",
    "        # print(tokenizer_de.decode(li2[0].to('cpu').numpy()),\n",
    "        #  tokenizer_de.decode(tgt[0].detach().to('cpu').numpy()))\n",
    "\n",
    "        loss = criterion(probs, labels)\n",
    "        # print(labels, probs, loss, labels.shape, probs.shape)       \n",
    "        # loss.backward)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        wandb.log(\n",
    "            {\"loss\": loss.data,\n",
    "             \"lr\": scheduler._rate})\n",
    "\n",
    "        # Optional\n",
    "        wandb.watch(net)\n",
    "\n",
    "        # break\n",
    "        # total += labels.size(0)\n",
    "        # correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if ep % num_steps == 0: \n",
    "#             val_accu = val(valloader)\n",
    "#             print(\"epoch {0} | loss: {1:.2f} | Val Accuracy: {2:.2f} %\".format(epoch, running_loss/num_steps, val_accu ))\n",
    "#             running_loss = 0.0\n",
    "#             if val_accu > best_accu:\n",
    "#               best_accu= val_accu \n",
    "#               torch.save(net.state_dict(), 'net_val.pth')\n",
    "#               print(\"Saving\")\n",
    "#             net.train()\n",
    "#     print(\"Train accuracy: {0:.2f} %\".format(100 * correct / total))\n",
    "\n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer_de_25000_start_token_SOS.json\")\n",
    "\n",
    "# for batch in loader_de.batch_iterator(1024):\n",
    "#     # print(batch)\n",
    "#     encoding = fast_tokenizer(batch)\n",
    "#     # decoded = fast_tokenizer.batch_decode(encoding['input_ids'])\n",
    "#     # print(decoded)\n",
    "#     # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('misc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1af58a0c390f807515830a18bbb19ac451fbe3aa00c4c733482807097ac6a02a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
